{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8129df66-9c50-48f2-a0f0-9f072f86d61c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (3058682447.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    filename = 'C:\\Users\\Sina\\.cache\\difraud\\fake_news\\test.jsonl'\u001b[0m\n\u001b[1;37m                                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "filename = 'test.jsonl'\n",
    "savename = 'test.csv'\n",
    "\n",
    "data = []\n",
    "with open(filename) as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "data2 = []\n",
    "for i in data:\n",
    "    data2.append(i['text'])\n",
    "\n",
    "data2 = pd.DataFrame(data2, columns=['text'])\n",
    "\n",
    "labels = []\n",
    "for i in data:\n",
    "    labels.append(i['label'])\n",
    "data2['label'] = labels\n",
    "data2.to_csv(savename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdc8b0a6-84ff-4ab8-be6f-a306a02229fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9b9c2f4-aa07-4120-a5f3-830a5ba93479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleLoader(Dataset):\n",
    "    def __init__(self, annotation_file):\n",
    "        super().__init__()\n",
    "        self.samples = pd.read_csv(annotation_file)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        txt = self.samples.iloc[idx]['text']\n",
    "        label = self.samples.iloc[idx]['label']\n",
    "        return txt, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91dfe75d-eba5-49e6-8004-c81e7a622feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SampleLoader(annotation_file='test.csv')\n",
    "trainloader = DataLoader(dataset=trainer, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60167a3a-52c4-416d-a495-9e7f5c962ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Police: Negotiators are now in contact with armed hostage-taker in Sydney situation; motive remains unknown - http://t.co/mp18E4DdjW', 'Muslims pray for the Friday prayer next to a sign reading #JeSuisCharlie in Saint-Etienne,France. By @JpKphotographer http://t.co/irukD1LedH', \"Police of #Ferguson murdered a boy in cold blood. Now they're spinning the story & intimidating the community into silence.\", \"JUST IN: Canadian military bases being closed after soldier shot at Nat'l War Memorial, ongoing situation near Parliament - @WilliamsJon\", \"Cowardly attack on Charlie Hebdo's offices. The humourless fanatics can fuck off. Freedom of speech is more important than your feelings.\", \"Airbus plane operated by Lufthansa's Germanwings airline crashes in southern France, officials say - @Reuters http://t.co/by471NAzFg\", '\"@LePoint: #CharlieHebdo : \"The cartoonists Charb & cabu are dead.\" http://t.co/7USD83cQd2 http://t.co/WGHbLiLFoX\"', 'Paris #CharlieHebdo attack: This is the worst act of terrorism in France for 50 years http://t.co/JO1hyB4vjj http://t.co/GPrxnlMx7a', 'Kudos to first responders working in Ottawa today. They are the real heroes. http://t.co/RsO1A2Fuka', 'UPDATE: Attacker shot dead in Parliament after soldier shot at War Memorial http://t.co/pp6hcfWcRw #OttawaShooting http://t.co/tx8j82FOYc', '\"Female Islamist terrorist\" attacks Paris Jewish store. As a woman, she has equal rights only when it comes to murdering nonbelievers.', 'Attempts to extend blame for this to all Muslims should be treated with the same disgust as attempts to justify the attacks. #CharlieHebdo', 'Names of pilots rumored to be Patrick Sonderheimer with co-pilot Andreas Lubitz - No formal confirmation yet. #germanwings', 'Hobby Lobby asks #Ferguson PD for clarification: \"We thought chopping off someone\\'s hand was the maximum penalty for shoplifting\"', '#Germanwings passenger plane crashes in French Alps with 148 onboard http://t.co/JCDZtv8We0 http://t.co/yE0hUNdiTr', '#4U9525: The age of the A320 (24 years) is not particularly unusual, though it is at the higher end of the curve.', 'Full statement from PM Abbott on #Sydneysiege #MartinPlace #Lindt. http://t.co/7ilKZGNBDz', '#Ferguson #BundyRanch #Hypocrisy / H/T to @Khary for the images. http://t.co/rt7u6zOA1p', '#BREAKING : Both Charlie Hebdo suspects killed as police storm building (police sources) http://t.co/OYSXRQ9xuv http://t.co/8ICThapo7z', '#Breaking: Hostage situation reportedly unfolding in #Sydney chocolate shop http://t.co/n4D3yGjso9', 'It\\'s a lot more quiet here at #qt as Capt. Johnson speaks. One guy just yelled \"we want answers.\" #ferguson #ksdk http://t.co/XpWHqqvzl6', 'And now reports of a possible fourth shooting incident at Trocadero near Eiffel Tower. Trying to get confirmation.', 'True cowardice: @nydailynews blurred out stereotypical imam in cover of #CharlieHebdo, left image of stereotypical Hasidic Jew.', 'Love. #ferguson https://t.co/CKUpWXYjkC', \"I'd be more inclined to take the #Ferguson PD account seriously if they hadn't been so careful to confiscate all video evidence beforehand.\", 'Ambulance workers rush injured out moments ago #sydneysiege http://t.co/UVQkTkPad7', 'The Terrograph, shame on you today. How low can you get spreading lies & misinformation? #auspol #SydneySiege http://t.co/KzbSPFx9K1', 'Technology can be used to expand democracy or enforce tyranny. Where we draw the line matters. #Ferguson http://t.co/d0DUaTczyW', 'Press has been corralled in with peaceful demonstrators in #ferguson http://t.co/UjGnf1hUhQ', 'Everything we know about the Germanwings plane crash in France http://t.co/o0ZSGb1lQq http://t.co/N4TTbxlOY4', \"I don't know if the #Ferguson police chief is acting out of malice or ignorance, but either way, he's the face the problem right now.\", 'French police are looking for two more suspects in the #Montrouge shooting: https://t.co/3GAC1myxzZ http://t.co/5wX2rPoGF5', 'Important things to remember during #sydneysiege - thanks to @filmvisuality http://t.co/MA73ZCspvd', 'The AFP have said the hostage taker is monitoring social media, according to Guardian Australia sources. #sydneysiege', 'UPDATE: There are reports police have discovered the identity of the lone gunman, with the #SydneySiege in its sixth hour. #9News', '-Muslim shooter - 1.7 Billion Muslims guilty -Black shooter - \"All Blacks are violent\" -White Shooter - \"Lone Wolf\" #CharlieHebdo', 'Update: Latest Images of the rescue operation of #4U9525.  (Pic via: CÃ©dric C./ActuSecours) http://t.co/ezktB3SXaA - @PollyR_Aviation', 'Press conference is over. Important to note: NO mention of #MikeBrown stealing ANYTHING from ANY store. #Ferguson http://t.co/CnX88sZEwY', 'Just in: Canadian soldier killed in #OttawaShooting today is Cpl. Nathan Cirillo Thoughts & prayers to his family http://t.co/fu2eZssBTr', 'WELP! RT @TUSK81: #Ferguson chief says officer Darren Wilson devastated. Mike Brown unavailable for comment about his feelings because dead.', 'Six explosions heard at kosher supermarket where a number of hostages were taken #ParisAttacks http://t.co/0jxoozr8mN http://t.co/BFuLAaMba6', 'High school graduation picture of Mike Brown. May he rest in peace, while we fight for his justice. #Ferguson http://t.co/ZpWHsvpnQi', '\"Charlie Hebdo: 10 truths that ought to be self-evident but aren\\'t\" - Me in @spectator  http://t.co/JgwY2xlmRn', 'News Corp seems to be having an internal conflict over whether we \"changed forever\" today or not. #SydneySiege http://t.co/X8QIjGxwAC', 'True north STRONG and free. Stay safe Ottawa. #PrayForOttawa #CanadaStrong ğŸ“•ğŸğŸ“• http://t.co/q5doY0TQ72', 'My gosh, horrible events in Ottawa. Thoughts and prayers. Why? Why? Why?', 'Meet Ahmed Merabet, first officer on scene to die at #CharlieHebdo , #ParisShooting http://t.co/D5z1suuB7P', 'This is what Paris looked like last night #charliehebdo http://t.co/HLeIy0187C http://t.co/LFqlElCl6C', \"And how are some of y'all so quick to believe the same police dept. that threw tear gas at folks in their gatdamb front yards. #Ferguson\", 'Looks like #Ferguson PD spent their whole week on a smear campaign against #MikeBrown instead of investigating or being at all helpful.', '9 Powerful Photos That Show #Ferguson Is Pretty Much Being Treated Like A War Zone http://t.co/LXKipJ2I6Q', '#JeSuisCharlie trends as social media users express solidarity over Charlie Hebdo attack http://t.co/4obxI0pj01 http://t.co/eJxcBHyPkE', 'Cartoon from The New Yorker, September 2012 #CharlieHebdo http://t.co/CeU2nZutSr', '#Germanwings change their logo to grey and white as a sign of mourning #4U9525 http://t.co/NK5U1GpHxL http://t.co/fZrvGy5p7w', \"Tonight, I spoke to President Hollande to convey Australia's solidarity, sympathy and friendship with the people of France #CharlieHebdo\", 'Thinking of everyone in Sydney. Hope you all get home safe and sound xx #PrayForSydney', 'Sad about #OttawaShooting. Worse, shooter might b guy who calls himself Muslim. Pls world dont judge 1.5 billion Muslims by some extremists.', 'Charlie Hebdo, we love you', 'UPDATE: Germanwings #A320 from Barcelona to Dusseldorf sent out a distress call at 1047 local time - @skymarkwhite', 'WATCH LIVE #CBCNN coverage of shooting in Ottawa Desktop: http://t.co/TNupeSXyNT Mobile: http://t.co/ruOo7HuPhL http://t.co/ZOmNWmOk28', 'These pro-Israel protesters with rifles in ATL werent ever approached by cops. It helps to be white #Ferguson http://t.co/0sDqDmJz8q', \"Wishing everyone in #Ottawa and all over Canada the best in the wake of today's shootings. I'll have a hug tomorrow for anyone who needs one\", '4 cartoonists who died in the attack on #CharlieHebdo - AFP http://t.co/UmyMvBKAa8', 'The #Germanwings co-pilot was not a Muslim, so he could not have been a terrorist. He was depressed. ğŸ˜’ http://t.co/ySZzpYHuWB', 'Gallows humor \"Give yourselves up or our elite cartoonist will draw a caricature of the Prophet\" #CharlieHebdo http://t.co/ea9y049Lqf', '10 people have died in a shooting at the Paris HQ of French weekly Charlie Hebdo, reports say. http://t.co/xcK8lY9IeR http://t.co/vsoysCGAbx', \"And I'm now officially done with @Uber, which is price gouging during AU hostage crisis. What sad sorry clowns. http://t.co/OTwTz3E8UM\", 'Freedom of expression should never be an act of bravery. Whoever threatens it is an enemy of all freedom-loving people. #CharlieHebdo', 'Why we need brave journalists: http://t.co/Oy2lnFafve #Ferguson', \"'Not Open For Discussion'â€”SWAT Tells Protesters In #Ferguson To Leave http://t.co/ULtXIi88Ah http://t.co/wLpoEPDnqJ (Photo: @BmoreConetta)\", 'BREAKING: Police source has told The Globe that a second shooter has been shot. #OttawaShooting', 'The name of the officer who shot #MikeBrown has been named. His name is Darren Wilson, a 6-year veteran of the #Ferguson police department.', 'What the hell is wrong with people? \"Taking selfies 100m from the Lindt Cafe #sydneysiege http://t.co/R7mNZCeSOD\"', 'From jaywalking to strong-arm robbery.  Mhm... #Ferguson', \"Islamic flag flown as 13 hostages in Sydney's CBD forced against windows http://t.co/qrADRShZqY http://t.co/EaR2kpGSqd\", 'FM #Steinmeier on #planecrash in #France: Terrible news. Our thoughts are with those who must fear for the lives of their relatives. #4U9525', 'Washington USA Seattle â†’ http://t.co/xPvOBDBRHv #CharlieHebdo 666 Satirical Newspaper Charlie Hebdo Braved Earlier Attack And Threats Beforâ€¦', 'BREAKING: CNN: Sydney police storm cafe. 2 confirmed dead. Syndey gunman is \"Muslim cleric.\" http://t.co/FBnfHqKkeP http://t.co/74z4Mp9WAV', 'Charlie Hebdo magazine is planning a print-run of its next issue of a million copies, not usual 60,000. Expected to sell out #wato', 'BREAKING NEWS: Killed Charlie Hebdo suspects came out firing on security forces. AFP', 'Video showing #CharlieHebdo attack. \"Allahu akbar\" shouts http://t.co/y1ErbSGv5P', 'Our thoughts are with the passengers and crew of flight 4U-9525 and all relatives and colleagues #unitedbywings @germanwings @lufthansa', \"Today at 1030GMT #police officers and staff around the country will pause to remember those killed in yesterday's #CharlieHebdo atrocity.\", \"St. Louis Police tells me they're currently talking to Dorian Johnson, key witness in #MikeBrown shooting http://t.co/f3DFtJO4jI #Ferguson\", 'Protestors have blocked West Florissant, a major street in St. Louis County. #Ferguson http://t.co/S15XpF4f9E', 'Hostages have reportedly been taken as police pursue gunmen responsible for the #ParisAttack http://t.co/RkQMx9cijy http://t.co/CilbwL4ewF', 'The worst of times can bring out the best in a lot of humanity. Except the super-rich. #JeSuisCharlie #CharlieHebdo http://t.co/ktqcjTFXPp', 'COUNTDOWN: One hour until #Ferguson police release the name of the officer that shot #MichaelBrown.', \"I normally don't do politics on here but totally shocked by the events of yesterday and heartened by the world's reaction #JeSuisCharlie\", 'Several hostages freed at Jewish supermarket in Paris including a child (R). Photo Thomas Samson #AFP http://t.co/b6ikL2BYyy', 'Our hearts go out to the people of Australia and all those affected by the #SydneySiege.', 'For those who, like me, wish Christooher Hitchens were here to comment, do read this: http://t.co/rJKcBzGk0r (via @elvis717) #JeSuisCharlie', 'BREAKING UPDATE: Soldier shot at National War Memorial in Ottawa, Canada, police tell @ABC News - http://t.co/wg7WHTjJGI', '11 dead in Paris shooting at French satirical newspaper Charlie Hebdo, police official says - @AP http://t.co/ZFJYh80uxy', 'Paris is on alert after gunmen killed twelve people at the office of satirical magazine Charlie Hebdo http://t.co/aUlvfkdg5u', \"Flag in window of Sydney Lindt cafe not an ISIS flag. Reads: 'There is no God but Allah and Muhammad is the messenger of God' (@liztilley84)\", 'Map showing the location of the #CharlieHebdo offices and the direction the attackers fled http://t.co/N28zmSCwVV http://t.co/q9EYxtcjJy', 'Mike Brown does not have to be perfect for his life to have value. #MikeBrown is not on trial. He is a homicide victim. #Ferguson', 'Paris Manhunt: 3 #CharlieHebdo attackers with AK-47 rifles & reportedly rocket-propelled grenade still at large http://t.co/3Jsosc7yl3', 'Shooting south of Paris with one police officer injured. Police say no known link to #CharlieHebdo attack http://t.co/UZlFI7Bb5s', 'Just received email message from @OttawaPolice Chief Charles Bordeleau #ottshooting #OttawaShooting http://t.co/4vJoP2YO11', \"#CharlieHebdo attack shows 'a complete change in terrorist methods', French FM @LaurentFabius tells me http://t.co/bYWd4RFFPk\", 'New vid of Paris attack on #CharlieHebdo filmed by a witness. Gunmen on street shout \"Allahu Akbar\" http://t.co/TaZEDtGFlW @akhbar', 'Police say 11 now dead in \"carnage\" at #CharlieHebdo HQ in Paris - follow our coverage: http://t.co/IClDjevRnB http://t.co/aQkN3PMfkt', \"Voltaire's gift: Why the battle for free speech is so important #CharlieHebdo http://t.co/7ioa4EHkju http://t.co/f9rwRMDani\", 'More police cars heading into #Ferguson https://t.co/cguoQrqjCn', \"BREAKING: Sydney terror suspect is Iranian Shi'ite, claims to have 'converted to Islam' a week ago on website. http://t.co/6Z5k6OlIDg\", '#BREAKING : Both hostages in #Dammartin and #Paris are free and safe http://t.co/OYSXRQ9xuv', 'LIVE: Sydney Opera House cancels performances in light of #sydneysiege events http://t.co/IKfAR0Z7HL http://t.co/jbL5Ngauvp', '90,000+ Tweets for #illridewithyou, 1,100 per minute. Trending number 1, globally. #SydneySiege http://t.co/NGeyUifhd8', 'DEVELOPING: Women, children held in second Paris hostage situation; may be related to massacre http://t.co/F4K3C1lot7', 'Why I am Charlie -- Parisians share emotional messages with CNN: http://t.co/YtQdjs8HQ1 #JeSuisCharlie https://t.co/3SGWCHVGg0', 'My thoughts are with the friends and family of the passengers of Germanwings flight 4U9525.', \"'Charlie Hebdo' editor killed in Paris terror attack built career on defiance http://t.co/T6VqPcerWe\", 'So basically #Ferguson PD hijacked what was supposed to be a press conference about Darren Wilson to call Michael Brown a criminal #NMOS14', \"Canada's thoughts and prayers are with our Australian friends. #MartinPlace #SydneySiege\", 'BREAKING NEWS #A320 crashed could be Germanwings flight #4U9525 from Barcelona to Dusseldorf. (140 passengers)', 'Sydney hostage crisis: 5 people have escaped, Islamic flag displayed. Our live coverage: http://t.co/CDzl7chb9R http://t.co/mJZHKcJY9e', 'Can confirm that several media outlets know the name and background of the hostage taker. Holding off publication. #sydneysiege', 'Hearing about the hostage situation in Sydney, I hope everything calms soon, and no one is harmed', 'Charlie Hebdo killers are dead. Sources say they are not in Paradise.', 'The Sydney attack gunman identified as Sheikh Man Haron Monis #SydneyCafeSiege #SydneySiege http://t.co/WUpPliupbK', \"This is why I don't give a ShitğŸ’©How Terrorist R interrogatedâŒ @benshapiro @HeyTammyBruce @MonicaCrowley #CharlieHebdo http://t.co/EEFH0zg7Qa\", \"RT @mkapral Today's French lesson. #JeSuisCharlie http://t.co/MXHkpKyLvC\", 'BREAKING Hostages held in Sydney, Australia, cafe with an Islamic flag displayed in window, according to local news: http://t.co/HdYk21TLGY', 'Seven people reportedly taken away on stretchers, with paramedics treating five. #SydneySiege http://t.co/mZ1o7Bi9kC http://t.co/QLsWpHSJTM', '#Ferguson police chief says he released the robbery video because he was getting FOIA requests for it. How did anyone know to request it?', \"Police aware that hostages are posting gunman's demands on social media. #sydneysiege\")\n",
      "tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for i , (text, label) in enumerate(trainloader):\n",
    "    print(text)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80c195ec-eaa9-43bb-998f-ffe05ca9828c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  Already, the mayor of Green Bay is having ribb...      1\n",
      "1  I argued for years that we need to move from a...      1\n",
      "2  The video doesn't show the broader picture of ...      0\n",
      "3  A ham and cheese sandwich on one slice of brea...      0\n",
      "4  He has been able to attract independents and D...      1\n",
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "val_data = pd.read_csv('validation.csv')\n",
    "\n",
    "# Inspect the data\n",
    "print(train_data.head())\n",
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2d25011-6729-4ed3-8971-2c20b03001be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sina\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_max_pooling1d            â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_max_pooling1d            â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - accuracy: 0.6870 - loss: 0.5880 - val_accuracy: 0.8549 - val_loss: 0.3410\n",
      "Epoch 2/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.9108 - loss: 0.2281 - val_accuracy: 0.8618 - val_loss: 0.3245\n",
      "Epoch 3/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.9706 - loss: 0.0918 - val_accuracy: 0.8463 - val_loss: 0.4451\n",
      "Epoch 4/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.9889 - loss: 0.0339 - val_accuracy: 0.8601 - val_loss: 0.4415\n",
      "Epoch 5/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.9952 - loss: 0.0170 - val_accuracy: 0.8584 - val_loss: 0.7168\n",
      "Epoch 6/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.9943 - loss: 0.0172 - val_accuracy: 0.8618 - val_loss: 0.6284\n",
      "Epoch 7/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.9972 - loss: 0.0156 - val_accuracy: 0.8549 - val_loss: 0.6232\n",
      "Epoch 8/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.9964 - loss: 0.0146 - val_accuracy: 0.8584 - val_loss: 0.6033\n",
      "Epoch 9/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step - accuracy: 0.9988 - loss: 0.0050 - val_accuracy: 0.8515 - val_loss: 0.6321\n",
      "Epoch 10/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.9985 - loss: 0.0044 - val_accuracy: 0.8601 - val_loss: 0.6729\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8686 - loss: 0.6653\n",
      "Test Accuracy: 86.36%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "val_data = pd.read_csv('validation.csv')\n",
    "\n",
    "# Define constants\n",
    "MAX_NUM_WORDS = 10000  # Use the top 10,000 most frequent words\n",
    "MAX_SEQUENCE_LENGTH = 200  # Maximum length of sequences\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_data['text'].values)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train = tokenizer.texts_to_sequences(train_data['text'].values)\n",
    "X_val = tokenizer.texts_to_sequences(val_data['text'].values)\n",
    "X_test = tokenizer.texts_to_sequences(test_data['text'].values)\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train = train_data['label'].values\n",
    "y_val = val_data['label'].values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=MAX_NUM_WORDS, output_dim=128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5))\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1cdebf0-cd68-4e6b-9e71-77f9cd40e7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sina\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ ?                      â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,109,200</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_max_pooling1d_1          â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ ?                      â”‚     \u001b[38;5;34m1,109,200\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_max_pooling1d_1          â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,109,200</span> (4.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,109,200\u001b[0m (4.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,109,200</span> (4.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,109,200\u001b[0m (4.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.6839 - loss: 0.6187 - val_accuracy: 0.7634 - val_loss: 0.4522\n",
      "Epoch 2/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7997 - loss: 0.4389 - val_accuracy: 0.7876 - val_loss: 0.4095\n",
      "Epoch 3/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8223 - loss: 0.4046 - val_accuracy: 0.8238 - val_loss: 0.3692\n",
      "Epoch 4/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.8431 - loss: 0.3465 - val_accuracy: 0.7824 - val_loss: 0.4552\n",
      "Epoch 5/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.8691 - loss: 0.3047 - val_accuracy: 0.8359 - val_loss: 0.3520\n",
      "Epoch 6/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8836 - loss: 0.2789 - val_accuracy: 0.8394 - val_loss: 0.3693\n",
      "Epoch 7/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9090 - loss: 0.2289 - val_accuracy: 0.8497 - val_loss: 0.3688\n",
      "Epoch 8/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.9097 - loss: 0.2247 - val_accuracy: 0.8325 - val_loss: 0.3903\n",
      "Epoch 9/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.9173 - loss: 0.1951 - val_accuracy: 0.8601 - val_loss: 0.3600\n",
      "Epoch 10/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9398 - loss: 0.1473 - val_accuracy: 0.8446 - val_loss: 0.3652\n",
      "Epoch 11/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9439 - loss: 0.1480 - val_accuracy: 0.8566 - val_loss: 0.3589\n",
      "Epoch 12/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.9376 - loss: 0.1592 - val_accuracy: 0.8428 - val_loss: 0.4238\n",
      "Epoch 13/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9536 - loss: 0.1308 - val_accuracy: 0.8394 - val_loss: 0.3738\n",
      "Epoch 14/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9591 - loss: 0.1075 - val_accuracy: 0.8394 - val_loss: 0.3860\n",
      "Epoch 15/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9621 - loss: 0.0949 - val_accuracy: 0.8428 - val_loss: 0.4133\n",
      "Epoch 16/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.9618 - loss: 0.1021 - val_accuracy: 0.8566 - val_loss: 0.4316\n",
      "Epoch 17/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.9697 - loss: 0.0857 - val_accuracy: 0.8394 - val_loss: 0.4686\n",
      "Epoch 18/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9667 - loss: 0.0863 - val_accuracy: 0.8532 - val_loss: 0.4687\n",
      "Epoch 19/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9774 - loss: 0.0770 - val_accuracy: 0.8480 - val_loss: 0.4865\n",
      "Epoch 20/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9695 - loss: 0.0884 - val_accuracy: 0.8497 - val_loss: 0.4941\n",
      "Epoch 21/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9742 - loss: 0.0804 - val_accuracy: 0.8601 - val_loss: 0.4935\n",
      "Epoch 22/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.9759 - loss: 0.0691 - val_accuracy: 0.8532 - val_loss: 0.5050\n",
      "Epoch 23/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9750 - loss: 0.0673 - val_accuracy: 0.8670 - val_loss: 0.4876\n",
      "Epoch 24/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9769 - loss: 0.0664 - val_accuracy: 0.8584 - val_loss: 0.4723\n",
      "Epoch 25/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9774 - loss: 0.0651 - val_accuracy: 0.8480 - val_loss: 0.5057\n",
      "Epoch 26/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.9736 - loss: 0.0659 - val_accuracy: 0.8480 - val_loss: 0.5225\n",
      "Epoch 27/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9789 - loss: 0.0529 - val_accuracy: 0.8549 - val_loss: 0.4884\n",
      "Epoch 28/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9844 - loss: 0.0459 - val_accuracy: 0.8394 - val_loss: 0.4983\n",
      "Epoch 29/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9785 - loss: 0.0575 - val_accuracy: 0.8497 - val_loss: 0.5304\n",
      "Epoch 30/30\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9764 - loss: 0.0545 - val_accuracy: 0.8618 - val_loss: 0.4520\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8407 - loss: 0.5868\n",
      "Test Accuracy: 84.80%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "val_data = pd.read_csv('validation.csv')\n",
    "\n",
    "# Define constants\n",
    "MAX_NUM_WORDS = 10000  # Use the top 10,000 most frequent words\n",
    "MAX_SEQUENCE_LENGTH = 200  # Maximum length of sequences\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_data['text'].values)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train = tokenizer.texts_to_sequences(train_data['text'].values)\n",
    "X_val = tokenizer.texts_to_sequences(val_data['text'].values)\n",
    "X_test = tokenizer.texts_to_sequences(test_data['text'].values)\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train = train_data['label'].values\n",
    "y_val = val_data['label'].values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(glove_file, word_index, embedding_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix\n",
    "\n",
    "# Path to the GloVe file\n",
    "glove_file = 'glove.6B.100d.txt'  # Change this to the actual path of the GloVe file\n",
    "embedding_dim = 100\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = load_glove_embeddings(glove_file, word_index, embedding_dim)\n",
    "\n",
    "# Define the CNN model with pre-trained embeddings and dropout layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False))\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5))\n",
    "model.add(Dropout(0.5))  # Add dropout layer\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Add dropout layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b5a7b16-3b78-4ebe-aa92-4dc359e5739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sina\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 142ms/step - accuracy: 0.7221 - loss: 0.5406 - val_accuracy: 0.7821 - val_loss: 0.4562\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 144ms/step - accuracy: 0.8143 - loss: 0.4096 - val_accuracy: 0.7793 - val_loss: 0.4486\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 143ms/step - accuracy: 0.8254 - loss: 0.3793 - val_accuracy: 0.8119 - val_loss: 0.4094\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 143ms/step - accuracy: 0.8585 - loss: 0.3342 - val_accuracy: 0.8253 - val_loss: 0.3896\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 150ms/step - accuracy: 0.8745 - loss: 0.2903 - val_accuracy: 0.8388 - val_loss: 0.3873\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 155ms/step - accuracy: 0.9105 - loss: 0.2265 - val_accuracy: 0.8436 - val_loss: 0.3842\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 155ms/step - accuracy: 0.9322 - loss: 0.1717 - val_accuracy: 0.8388 - val_loss: 0.3695\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 159ms/step - accuracy: 0.9544 - loss: 0.1360 - val_accuracy: 0.8349 - val_loss: 0.4373\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 162ms/step - accuracy: 0.9693 - loss: 0.0987 - val_accuracy: 0.8532 - val_loss: 0.4594\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 163ms/step - accuracy: 0.9794 - loss: 0.0653 - val_accuracy: 0.8647 - val_loss: 0.4846\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - accuracy: 0.8693 - loss: 0.4665\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 164ms/step - accuracy: 0.7266 - loss: 0.5487 - val_accuracy: 0.7985 - val_loss: 0.4250\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 158ms/step - accuracy: 0.8120 - loss: 0.4182 - val_accuracy: 0.8186 - val_loss: 0.3901\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 158ms/step - accuracy: 0.8447 - loss: 0.3481 - val_accuracy: 0.8445 - val_loss: 0.3414\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 164ms/step - accuracy: 0.8745 - loss: 0.2883 - val_accuracy: 0.8378 - val_loss: 0.3731\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 163ms/step - accuracy: 0.8836 - loss: 0.2680 - val_accuracy: 0.8570 - val_loss: 0.3315\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 165ms/step - accuracy: 0.9293 - loss: 0.1804 - val_accuracy: 0.8589 - val_loss: 0.3306\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 161ms/step - accuracy: 0.9547 - loss: 0.1411 - val_accuracy: 0.8589 - val_loss: 0.3578\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 159ms/step - accuracy: 0.9658 - loss: 0.1027 - val_accuracy: 0.8704 - val_loss: 0.3717\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 161ms/step - accuracy: 0.9790 - loss: 0.0732 - val_accuracy: 0.8628 - val_loss: 0.4073\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 168ms/step - accuracy: 0.9885 - loss: 0.0519 - val_accuracy: 0.8599 - val_loss: 0.4398\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.8479 - loss: 0.4826\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 168ms/step - accuracy: 0.7267 - loss: 0.5376 - val_accuracy: 0.8042 - val_loss: 0.4229\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 169ms/step - accuracy: 0.8166 - loss: 0.4102 - val_accuracy: 0.8081 - val_loss: 0.4031\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 172ms/step - accuracy: 0.8325 - loss: 0.3792 - val_accuracy: 0.8109 - val_loss: 0.3829\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 170ms/step - accuracy: 0.8576 - loss: 0.3348 - val_accuracy: 0.8167 - val_loss: 0.4105\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.8814 - loss: 0.2796 - val_accuracy: 0.8397 - val_loss: 0.3526\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 164ms/step - accuracy: 0.9029 - loss: 0.2420 - val_accuracy: 0.8417 - val_loss: 0.3369\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 169ms/step - accuracy: 0.9337 - loss: 0.1766 - val_accuracy: 0.8445 - val_loss: 0.3558\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 173ms/step - accuracy: 0.9564 - loss: 0.1324 - val_accuracy: 0.8541 - val_loss: 0.3810\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 169ms/step - accuracy: 0.9672 - loss: 0.1015 - val_accuracy: 0.8417 - val_loss: 0.4619\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 169ms/step - accuracy: 0.9819 - loss: 0.0665 - val_accuracy: 0.8455 - val_loss: 0.4431\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.8384 - loss: 0.4974\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 169ms/step - accuracy: 0.7000 - loss: 0.5563 - val_accuracy: 0.8301 - val_loss: 0.3967\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 181ms/step - accuracy: 0.7934 - loss: 0.4376 - val_accuracy: 0.8417 - val_loss: 0.3674\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 169ms/step - accuracy: 0.8266 - loss: 0.3844 - val_accuracy: 0.8474 - val_loss: 0.3536\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 171ms/step - accuracy: 0.8609 - loss: 0.3228 - val_accuracy: 0.8551 - val_loss: 0.3330\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 170ms/step - accuracy: 0.8805 - loss: 0.2877 - val_accuracy: 0.8772 - val_loss: 0.3115\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 171ms/step - accuracy: 0.9033 - loss: 0.2393 - val_accuracy: 0.8704 - val_loss: 0.3129\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 163ms/step - accuracy: 0.9368 - loss: 0.1741 - val_accuracy: 0.8695 - val_loss: 0.3369\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 163ms/step - accuracy: 0.9568 - loss: 0.1284 - val_accuracy: 0.8599 - val_loss: 0.3521\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 169ms/step - accuracy: 0.9641 - loss: 0.1025 - val_accuracy: 0.8599 - val_loss: 0.3984\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 171ms/step - accuracy: 0.9831 - loss: 0.0615 - val_accuracy: 0.8599 - val_loss: 0.4497\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.8552 - loss: 0.4535\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 177ms/step - accuracy: 0.7155 - loss: 0.5514 - val_accuracy: 0.7860 - val_loss: 0.4917\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 164ms/step - accuracy: 0.8035 - loss: 0.4275 - val_accuracy: 0.8138 - val_loss: 0.4155\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 163ms/step - accuracy: 0.8242 - loss: 0.3787 - val_accuracy: 0.7975 - val_loss: 0.4305\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.8508 - loss: 0.3303 - val_accuracy: 0.8157 - val_loss: 0.4296\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.8839 - loss: 0.2819 - val_accuracy: 0.8244 - val_loss: 0.4041\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 172ms/step - accuracy: 0.9094 - loss: 0.2273 - val_accuracy: 0.8388 - val_loss: 0.4016\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 168ms/step - accuracy: 0.9292 - loss: 0.1886 - val_accuracy: 0.8493 - val_loss: 0.3711\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 163ms/step - accuracy: 0.9620 - loss: 0.1191 - val_accuracy: 0.8512 - val_loss: 0.3975\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 161ms/step - accuracy: 0.9748 - loss: 0.0853 - val_accuracy: 0.8560 - val_loss: 0.4344\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 161ms/step - accuracy: 0.9833 - loss: 0.0632 - val_accuracy: 0.8417 - val_loss: 0.4652\n",
      "\u001b[1m33/33\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.8460 - loss: 0.4640\n",
      "Average Cross-Validation Accuracy: 85.43%\n",
      "Epoch 1/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 151ms/step - accuracy: 0.7266 - loss: 0.5327\n",
      "Epoch 2/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 159ms/step - accuracy: 0.8168 - loss: 0.4041\n",
      "Epoch 3/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 152ms/step - accuracy: 0.8375 - loss: 0.3702\n",
      "Epoch 4/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 148ms/step - accuracy: 0.8684 - loss: 0.3118\n",
      "Epoch 5/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 148ms/step - accuracy: 0.8946 - loss: 0.2540\n",
      "Epoch 6/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 153ms/step - accuracy: 0.9185 - loss: 0.2037\n",
      "Epoch 7/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 154ms/step - accuracy: 0.9403 - loss: 0.1564\n",
      "Epoch 8/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 149ms/step - accuracy: 0.9699 - loss: 0.0986\n",
      "Epoch 9/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 147ms/step - accuracy: 0.9756 - loss: 0.0803\n",
      "Epoch 10/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 147ms/step - accuracy: 0.9882 - loss: 0.0477\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - accuracy: 0.8609 - loss: 0.4515\n",
      "Test Accuracy: 85.15%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, GlobalMaxPooling1D, Dense, Dropout\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "val_data = pd.read_csv('validation.csv')\n",
    "\n",
    "# Combine train and validation data for cross-validation\n",
    "combined_data = pd.concat([train_data, val_data])\n",
    "\n",
    "# Define constants\n",
    "MAX_NUM_WORDS = 10000  # Use the top 10,000 most frequent words\n",
    "MAX_SEQUENCE_LENGTH = 200  # Maximum length of sequences\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(combined_data['text'].values)\n",
    "\n",
    "# Convert text to sequences\n",
    "X = tokenizer.texts_to_sequences(combined_data['text'].values)\n",
    "X_test = tokenizer.texts_to_sequences(test_data['text'].values)\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y = combined_data['label'].values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(glove_file, word_index, embedding_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix\n",
    "\n",
    "# Path to the GloVe file\n",
    "glove_file = 'glove.6B.100d.txt'  # Change this to the actual path of the GloVe file\n",
    "embedding_dim = 100\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = load_glove_embeddings(glove_file, word_index, embedding_dim)\n",
    "\n",
    "# Function to create the Bidirectional LSTM model\n",
    "def create_bilstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=MAX_SEQUENCE_LENGTH,\n",
    "                        trainable=False))\n",
    "    model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Perform K-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    model = create_bilstm_model()\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_val, y_val)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f'Average Cross-Validation Accuracy: {average_accuracy*100:.2f}%')\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "model = create_bilstm_model()\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n",
    "loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf66f5b-0876-4b53-b8cc-9ecc8547ba34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63052e8a71244779a63b85e74a2c5956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  14%|#4        | 62.9M/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('validation.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Define constants\n",
    "MAX_NUM_WORDS = 10000  # Use the top 10,000 most frequent words\n",
    "MAX_SEQUENCE_LENGTH = 200  # Maximum length of sequences\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_data['text'].values)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train = tokenizer.texts_to_sequences(train_data['text'].values)\n",
    "X_val = tokenizer.texts_to_sequences(val_data['text'].values)\n",
    "X_test = tokenizer.texts_to_sequences(test_data['text'].values)\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train = train_data['label'].values\n",
    "y_val = val_data['label'].values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=MAX_NUM_WORDS, output_dim=128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be102f6-5653-4c49-afff-be45b0f90a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
